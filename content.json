{"meta":{"title":"虫虫特工队","subtitle":"","description":"","author":"Summ张张","url":"http://yoursite.com","root":"/"},"pages":[{"title":"archives","date":"2019-12-10T12:19:15.000Z","updated":"2019-12-10T12:19:15.815Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"about","date":"2019-12-10T07:05:43.000Z","updated":"2019-12-10T14:12:41.536Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"我是张舒萌 欢迎来到我的博客—虫虫特工队"},{"title":"tags","date":"2019-12-10T06:57:06.000Z","updated":"2019-12-10T14:07:43.828Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"friends","date":"2019-12-10T12:34:34.000Z","updated":"2019-12-10T13:33:43.386Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-12-10T07:05:14.000Z","updated":"2019-12-10T14:10:04.438Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"回文数","slug":"回文数","date":"2019-12-11T02:30:09.020Z","updated":"2019-12-11T10:55:11.686Z","comments":true,"path":"2019/12/11/回文数/","link":"","permalink":"http://yoursite.com/2019/12/11/%E5%9B%9E%E6%96%87%E6%95%B0/","excerpt":"","text":"若一个数（首位不为零）从左向右读与从右向左读都一样，我们就将其称之为回文数。 示例 1: 1输入: 1212输出: true 示例 2: 1输入: -1212输出: false3解释: 从左向右读, 为 -121 。 从右向左读, 为 121- 。因此它不是一个回文数。 示例 3: 1输入: 102输出: false3解释: 从右向左读, 为 01 。因此它不是一个回文数。 ==两种解法== 解法1:转换成字符串进行操作 解法2:所以这个解法的操作就是 取出后半段数字进行翻转。 这里需要注意的一个点就是由于回文数的位数可奇可偶，所以当它的长度是偶数时，它对折过来应该是相等的；当它的长度是奇数时，那么它对折过来后，有一个的长度需要去掉一位数（除以 10 并取整）。 具体做法如下： 每次进行取余操作 （ %10），取出最低的数字：y = x % 10将最低的数字加到取出数的末尾：revertNum = revertNum * 10 + y每取一个最低位数字，x 都要自除以 10判断 x 是不是小于 revertNum ，当它小于的时候，说明数字已经对半或者过半了最后，判断奇偶数情况：如果是偶数的话，revertNum 和 x 相等；如果是奇数的话，最中间的数字就在revertNum 的最低位上，将它除以 10 以后应该和 x 相等。","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/categories/leetcode/"}],"tags":[]},{"title":"二叉搜索树的范围和","slug":"二叉搜索树的范围和","date":"2019-12-11T02:30:06.212Z","updated":"2019-12-11T02:39:31.309Z","comments":true,"path":"2019/12/11/二叉搜索树的范围和/","link":"","permalink":"http://yoursite.com/2019/12/11/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E8%8C%83%E5%9B%B4%E5%92%8C/","excerpt":"","text":"二叉搜索树的定义二叉搜索树，也称有序二叉树,排序二叉树，是指一棵空树或者具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树。 没有键值相等的节点。 题目：给定二叉搜索树的根结点 root，返回 L 和 R（含）之间的所有结点的值的和。 二叉搜索树保证具有唯一的值。 示例 1： 1输入：root &#x3D; [10,5,15,3,7,null,18], L &#x3D; 7, R &#x3D; 152输出：32 示例 2： 1输入：root &#x3D; [10,5,15,3,7,13,18,1,null,6], L &#x3D; 6, R &#x3D; 102输出：23 提示： 1树中的结点数量最多为 10000 个。2最终的答案保证小于 2^31。","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/categories/leetcode/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"http://yoursite.com/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"链表","slug":"链表","permalink":"http://yoursite.com/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"合并二叉树","slug":"合并二叉树","date":"2019-12-11T02:30:02.579Z","updated":"2019-12-11T02:39:37.047Z","comments":true,"path":"2019/12/11/合并二叉树/","link":"","permalink":"http://yoursite.com/2019/12/11/%E5%90%88%E5%B9%B6%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。 你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。 示例 1: 输入: 1Tree 1 Tree 2 2 1 2 3 &#x2F; \\ &#x2F; \\ 4 3 2 1 3 5 &#x2F; \\ \\ 6 5 4 7 输出: 1合并后的树:2 33 &#x2F; \\4 4 55 &#x2F; \\ \\ 6 5 4 7 注意: 合并必须从两个树的根节点开始。 如果1子树为空，则返回2子树，如果2子树为空，则返回1子树，若都不为空，则相加节点的值，利用递归同时进行遍历。 1&#x2F;**2 * Definition for a binary tree node.3 * struct TreeNode &#123;4 * int val;5 * struct TreeNode *left;6 * struct TreeNode *right;7 * &#125;;8 *&#x2F;91011struct TreeNode* mergeTrees(struct TreeNode* t1, struct TreeNode* t2)&#123;12 if(t2&#x3D;&#x3D;NULL) return t1;13 else if(t1&#x3D;&#x3D;NULL) return t2;14 t1-&gt;val+&#x3D;t2-&gt;val;15 t1-&gt;left&#x3D;mergeTrees(t1-&gt;left,t2-&gt;left);16 t1-&gt;right&#x3D;mergeTrees(t1-&gt;right,t2-&gt;right);17 return t1;18&#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/categories/leetcode/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"http://yoursite.com/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"链表","slug":"链表","permalink":"http://yoursite.com/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"基于中断循环神经网络的文本分类","slug":"科大讯飞：基于中断循环神经网络的文本分类","date":"2019-12-11T02:22:37.070Z","updated":"2019-12-11T02:40:08.461Z","comments":true,"path":"2019/12/11/科大讯飞：基于中断循环神经网络的文本分类/","link":"","permalink":"http://yoursite.com/2019/12/11/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%EF%BC%9A%E5%9F%BA%E4%BA%8E%E4%B8%AD%E6%96%AD%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/","excerpt":"","text":"ACL 2018 ：科大讯飞：基于中断循环神经网络的文本分类原文:Disconnected Recurrent Neural Networks for Text Categorization 基于中断循环神经网络的文本分类 科大讯飞北京研究院HFL实验室 引言文本分类任务是自然语言处理（NLP）领域最基础和传统的任务之一，该任务又会根据领域类型的不同分成很多子任务，例如情感分类、主题分类和问题分类等。很多机器学习的新方法都会先尝试在文本分类任务上进行实验验证。例如深度学习中最常见的两大类模型，卷积神经网络（CNN）和循环神经网络（RNN）及其变体，在文本分类中有很多应用。 RNN模型擅长对整个句子进行建模，捕捉长距离依赖信息。然而研究表明，RNN对整个句子建模有时会成为一种负担，使模型忽略了关键的短语信息。CNN模型则正相反，更擅长抽取局部的位置不变特征，而不擅长捕捉长距离依赖信息。为此，我们提出了DRNN模型，通过限制RNN模型信息的流动，将位置不变性引入RNN模型中。这使得DRNN模型既能捕捉长距离依赖信息，又可以很好地抽取关键短语信息。我们提出的模型在DBPedia，Yelp等多个文本分类数据集上取得了最好的效果。 模型介绍表1是一个主题分类的例子，我们可以看到两句话表意基本相同，都应该被分类到科技类。其中决定分类的关键短语是“unsolved mysteries of mathematics”，对于一个窗口大小为4的CNN模型来说，两个句子中的该短语表示完全相同。然而，当我们把两句话送入RNN模型的时候，因为RNN模型中，每一个时刻的隐层状态都和前面所有词语相关，所以这两个短语的表示是完全不同的。这增大了模型捕捉关键短语的难度，使得RNN模型有时会忽视一些关键信息。 表格1 主题分类示例 为了解决上述问题，我们希望通过将位置不变性引入RNN模型中，使得RNN模型既可以捕捉长距离依赖信息，又可以更好地抽取位置不变的局部特征。具体来说，我们会阻断RNN模型的信息流动，使其最多只能传递固定的步长k。这样的话，每个时刻的隐层状态就只和当前词以及前k-1个词相关。 图1 Disconnected Recurrent Neural Networks 图1是RNN模型、DRNN模型和CNN模型的一个对比图。如图所示，对于RNN来说，隐层状态h与前面所有的词都相关，而对于DRNN，则只与当前词及之前的k-1个词相关。DRNN模型也可以被认为是一种特殊的CNN模型，只是将CNN模型中的卷积核替换成了RNN。显然，DRNN和CNN一样，对于长度为k的短语，无论它在文本中的什么位置，都具有相同的表示。DRNN模型t时刻的隐藏层输出可以表示成如下形式： DRNN是一种通用的模型框架，可以应用在很多任务中，我们主要将其应用在文本分类任务中，对应的模型结构见图2。我们采用GRU作为DRNN的循环单元，得到Disconnected Gated Recurrent Unit（DGRU）模型。我们首先将DGRU的每个隐层表示送入MLP中，来抽取更高层的特征信息。然后再通过Max Pooling来抽取整个文本中最重要的信息，最后再通过一层MLP，然后送入softmax中进行分类。","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"文本分类","slug":"文本分类","permalink":"http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"}]},{"title":"关于卷积","slug":"关于卷积","date":"2019-12-11T02:14:57.818Z","updated":"2019-12-11T02:40:02.494Z","comments":true,"path":"2019/12/11/关于卷积/","link":"","permalink":"http://yoursite.com/2019/12/11/%E5%85%B3%E4%BA%8E%E5%8D%B7%E7%A7%AF/","excerpt":"","text":"对卷积的困惑教科书上一般定义函数f,g的卷积f*g(n)如下： 连续形式 离散形式 对卷积的理解整体看来是这么个过程： 翻转——&gt;滑动——&gt;叠加——&gt;滑动——&gt;叠加——&gt;滑动——&gt;叠加….. 多次滑动得到的一系列叠加值，构成了卷积函数。 例1⃣：信号分析如下图所示，输入信号是 f(t) ，是随时间变化的。系统响应函数是 g(t) ，图中的响应函数是随时间指数下降的，它的物理意义是说：如果在 t=0 的时刻有一个输入，那么随着时间的流逝，这个输入将不断衰减。换言之，到了 t=T时刻，原来在 t=0 时刻的输入f(0)的值将衰减为f(0)g(T)。 考虑到==信号是连续输入的，也就是说，每个时刻都有新的信号进来，所以，最终输出的是所有之前输入信号的累积效果==。如下图所示，在T=10时刻，输出结果跟图中带标记的区域整体有关。其中，f(10)因为是刚输入的，所以其输出结果应该是f(10)g(0)，而时刻t=9的输入f(9)，只经过了1个时间单位的衰减，所以产生的输出应该是f(9)g(1)，如此类推，即图中虚线所描述的关系。这些对应点相乘然后累加，就是T=10时刻的输出信号值，这个结果也是f和g两个函数在T=10时刻的卷积值。 累积效果可以将g函数对折一下，变成g(-t)，这就是为什么卷积要“卷”，要翻转的原因，这是从它的物理意义中给出的。然后再进行T个单位的平移就可以得到所以，在以上计算T时刻的卷积时，要维持的约束就是： t+ (T-t) = T 例2⃣：丢骰子要解决的问题是：有两枚骰子，把它们都抛出去，两枚骰子点数加起来为4的概率是多少? ​​分析一下，两枚骰子点数加起来为4的情况有三种情况：1+3=4， 2+2=4, 3+1=4因此，两枚骰子点数加起来为4的概率为： 写成卷积的方式就是：​ 还用反转滑动叠加的逻辑进行解释，将函数g翻转，然后阴影区域上下对应的数相乘，然后累加，相当于求自变量为4的卷积值，如下图所示：​​进一步，如此翻转以后，可以方便地进行推广去求两个骰子点数和为 n 时的概率，为f 和 g的卷积 f*g(n)，如下图所示：​由上图可以看到，函数 g 的滑动，带来的是点数和的增大。这个例子中对f和g的约束条件就是点数和，它也是卷积函数的自变量。有兴趣还可以算算，如果骰子的每个点数出现的概率是均等的，那么两个骰子的点数和n=7的时候，概率最大。 由上图可以看到，函数 g 的滑动，带来的是点数和的增大。这个例子中对f和g的约束条件就是点数和，它也是卷积函数的自变量。有兴趣还可以算算，如果骰子的每个点数出现的概率是均等的，那么两个骰子的点数和n=7的时候，概率最大。","categories":[{"name":"基础支撑","slug":"基础支撑","permalink":"http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E6%94%AF%E6%92%91/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-12-09T09:35:33.362Z","updated":"2019-12-11T02:40:14.747Z","comments":true,"path":"2019/12/09/hello-world/","link":"","permalink":"http://yoursite.com/2019/12/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"test","slug":"test","permalink":"http://yoursite.com/categories/test/"}],"tags":[]},{"title":"对话中的情感识别","slug":"【论文阅读】DialogueGCN 图神经网络在对话中进行情感识别","date":"2019-12-08T09:32:27.679Z","updated":"2019-12-11T02:39:43.135Z","comments":true,"path":"2019/12/08/【论文阅读】DialogueGCN 图神经网络在对话中进行情感识别/","link":"","permalink":"http://yoursite.com/2019/12/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91DialogueGCN%20%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%AF%B9%E8%AF%9D%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/","excerpt":"","text":"EMNLP 2019 DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation原文pdf 摘要 Emotion recognition in conversation (ERC) has received much attention, lately, from re- searchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage ==self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition==. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets. 研究背景—什么是情感识别？最近，深度学习在自然语言处理领域（NLP）取得了很大的进步。随着诸如 Attention 和 Transformers 之类新发明的出现，BERT 和 XLNet 一次次取得进步，使得文本情感识别之类的等任务变得更加容易。本文将介绍一种新的方法，该方法使用图模型在对话中进行情感识别。 什么是情感识别？简而言之，情感识别（ERC）是对文字背后的情感进行分类的任务。例如，给定一段文字，你能说出说话者是生气、快乐、悲伤还是困惑吗？情感识别在医疗保健、教育、销售和人力资源方面具有许多广泛的应用。从最高的一个层面讲，情感识别任务非常有用，因为许多人认为，这是构建能够与人类对话的智能 AI 的基石。","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"情感分析","slug":"情感分析","permalink":"http://yoursite.com/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"}]}]}